/**
 * @fileoverview Benchmark Runner Script (Node.js)
 * 
 * This script is executed as a child process by the BenchmarkService to run performance 
 * benchmarks on different JavaScript/TypeScript function implementations using the 'benny' library.
 * 
 * It receives the path to a temporary file (`functionsFilePath`) as a command-line argument.
 * This file contains:
 *   - `testData`: The input data generated by the LLM for the specific function signature.
 *   - `implementations`: An object where keys are implementation names (e.g., 'Original', 
 *     'Alternative_1') and values are the stringified code of each function implementation.
 * 
 * CRITICAL LOGIC FOR ACCURATE BENCHMARKING:
 * ------------------------------------------
 * To ensure that Benny measures the execution time of the function call itself, rather than 
 * the overhead of setting up the execution environment (like vm context creation and code parsing),
 * this script performs two crucial steps *before* starting the Benny suite:
 * 
 * 1. Argument Determination (`argsForRun`):
 *    - The script analyzes the `testData` loaded from the temporary file ONCE.
 *    - It determines the exact arguments needed for the benchmark function based on the 
 *      structure of `testData` (handling specific known structures like the one for 
 *      `findAllMatchingExpoResolutionPathsOld` and a default case for other functions 
 *      like `processNumbers`).
 *    - This avoids repeated argument parsing/determination within the timed loop.
 * 
 * 2. Function Pre-compilation (`preparedFunctions` Map):
 *    - The script iterates through each function implementation string provided.
 *    - For each implementation, it creates a `vm` sandbox context ONCE.
 *    - It executes the function code string within the `vm` context ONCE to define the function.
 *    - It retrieves a reference to the compiled function from the `vm` context ONCE.
 *    - This compiled function reference is stored in the `preparedFunctions` map.
 *    - This avoids the extremely expensive overhead of parsing and compiling the function 
 *      code string on every iteration of Benny's measurement cycle.
 * 
 * Benny Suite Execution (`benny.add`):
 *    - Inside the `benny.add((...) => { ... })` callback, which Benny runs potentially millions 
 *      of times for measurement:
 *      - The pre-compiled function reference is retrieved from the `preparedFunctions` map.
 *      - The pre-determined arguments (`argsForRun`) are used.
 *      - The function is called directly (`funcToRun(...argsForRun)`).
 *    - This ensures the timed portion *only* contains the function call itself, leading to 
 *      accurate and high ops/sec measurements.
 * 
 * IMPORTANCE:
 * ----------- 
 * Modifying or removing the argument determination or function pre-compilation logic 
 * will likely reintroduce significant overhead into the benchmark loop, drastically reducing 
 * the reported ops/sec and making the performance results inaccurate and misleading. 
 * This setup is ESSENTIAL for the benchmark runner to function correctly.
 */

// src/utils/benchmarkRunner.js

// This script runs the Benny benchmark using pre-generated function/data file.

const benny = require('benny');
const path = require('path');
const fs = require('fs');
const vm = require('vm');

// !!! CRITICAL SECTION START: BENCHMARK ACCURACY SETUP !!!
// The following sections (Argument Determination, Function Pre-compilation) 
// are ESSENTIAL for accurate benchmarking. Modifying them without 
// understanding the impact WILL likely invalidate performance results by
// including setup overhead (parsing, vm context creation) in the timed execution.
// !!! CRITICAL SECTION END: BENCHMARK ACCURACY SETUP !!!

// +++ START REFACTOR HELPER FUNCTION +++
/**
 * Determines the arguments array to be passed to the benchmarked functions
 * based on the structure of the provided testData.
 * Handles known specific structures and provides a default.
 * CRITICAL: This avoids repeated, expensive argument determination inside
 *           Benny's timed measurement loop.
 * @param testData - The test data loaded from the benchmark file.
 * @returns An array of arguments to be used for function calls.
 * @throws Error if argument determination fails unexpectedly.
 */
// Export the function for testing
export function determineArguments(testData: any): any[] {
    try {
        // Check for the specific known structure of findAllMatchingExpoResolutionPathsOld testData
        if (typeof testData === 'object' && 
            testData !== null && 
            !Array.isArray(testData) && 
            testData.hasOwnProperty('indexMapping') && 
            testData.hasOwnProperty('resolutionInfo')) 
        {
            // Specific case for findAll... which takes 2 object arguments
            console.log('[BenchmarkRunner ArgHelper] Detected specific structure for findAll...');
            return [testData.indexMapping, testData.resolutionInfo];
        } else {
            // Default case: Assume testData represents a SINGLE argument for the benchmark function.
            // This works for processNumbers where testData is the array argument itself.
            // This would also work if testData was a single object or primitive.
            // This might fail if testData is an array meant to be spread as multiple arguments (e.g., testData = [5, 10] for add(a,b)) -
            // requires LLM to provide testData appropriately based on function signature.
            console.log('[BenchmarkRunner ArgHelper] Using default: testData as single argument.');
            return [testData];
        }
    } catch (argError) {
        // Log the error with context
        console.error(`[BenchmarkRunner ArgHelper DEBUG] Caught error during argument determination: ${argError}`);
        // Re-throw or handle as needed - here we let the main function catch it
        throw new Error(`Failed to determine arguments from testData: ${argError}`);
    }
}
// +++ END REFACTOR HELPER FUNCTION +++

// +++ START REFACTOR HELPER FUNCTION: Compile Implementations +++
/**
 * Compiles function implementation strings using Node.js vm module.
 * Creates an isolated context for each function and retrieves a reference.
 * CRITICAL: Pre-compiles functions ONCE to avoid overhead in Benny's timing loop.
 * @param implementations - Object mapping implementation names to code strings.
 * @param implementationKeys - Array of keys (names) from the implementations object.
 * @returns A Map where keys are implementation names and values are the compiled function references.
 * @throws Error if compilation fails for any implementation.
 */
export function compileImplementations(
    implementations: Record<string, string>,
    implementationKeys: string[]
): Map<string, (...args: any[]) => any> {
    const preparedFunctions = new Map<string, (...args: any[]) => any>();
    console.log('[BenchmarkRunner CompilerHelper] Pre-compiling functions...');

    for (const implKey of implementationKeys) {
        try {
            const context = {
                 // Basic safe globals
                 console: { log: () => {}, warn: () => {}, error: () => {} }, // Prevent benchmarked code logging
                 math: Math,
                 // Add other safe globals if needed, but avoid anything complex/stateful
            };
            vm.createContext(context);

            // Run the code string to define the function within the context
            vm.runInContext(implementations[implKey], context, { timeout: 1000 }); // Added timeout

            // Retrieve the actual function reference from the context using its key (name)
            const funcRef = vm.runInContext(implKey, context);

            if (typeof funcRef !== 'function') {
                throw new Error(`Implementation '${implKey}' did not evaluate to a function.`);
            }
            preparedFunctions.set(implKey, funcRef);
            console.log(`[BenchmarkRunner CompilerHelper] Successfully compiled: ${implKey}`);
        } catch (compileError: any) {
            // Add more context to the error message
            const errMsg = `BENCHMARK_ERROR: Failed to compile function '${implKey}': ${compileError.message || compileError}`;
            console.error(errMsg);
             // Rethrow a new error to be caught by the main runBenchmarks function
            throw new Error(errMsg); 
        }
    }
    console.log('[BenchmarkRunner CompilerHelper] All functions pre-compiled.');
    return preparedFunctions;
}
// +++ END REFACTOR HELPER FUNCTION +++

// Wrap the main logic in an async function to allow awaiting Benny's completion
// Export for testing
export async function runBenchmarks(functionsFilePath: string, _mockModule?: Record<string, any>) {
    // +++ DEBUG LOG +++
    console.log(`[BenchmarkRunner DEBUG] Checking existence of: ${functionsFilePath}`);

    // Revert back to synchronous existsSync check
    if (!fs.existsSync(functionsFilePath)) {
        // +++ DEBUG LOG +++
        console.error(`[BenchmarkRunner DEBUG] fs.existsSync returned false for: ${functionsFilePath}`);
        console.error(`BENCHMARK_ERROR: Functions file not found: ${functionsFilePath}`);
        process.exit(1);
    }

    // +++ DEBUG LOG +++
    console.log(`[BenchmarkRunner DEBUG] File exists. Attempting require: ${path.resolve(functionsFilePath)}`);

    console.log('[BenchmarkRunner] Required functions file successfully:', functionsFilePath);

    let loadedModule: Record<string, any>;

    // --- Module Loading --- 
    if (_mockModule) {
        console.log('[BenchmarkRunner DEBUG] Using provided mock module.');
        loadedModule = _mockModule;
    } else {
        try {
            // Require the dynamically generated file
            const requiredModule = require(path.resolve(functionsFilePath));
            // +++ DEBUG LOG +++
            console.log(`[BenchmarkRunner DEBUG] Successfully required module from: ${functionsFilePath}`);
            // Basic type check after require
            if (typeof requiredModule !== 'object' || requiredModule === null) {
                throw new Error('Module did not export an object.');
            }
            loadedModule = requiredModule as Record<string, any>;
        } catch (error) {
            console.error(`BENCHMARK_ERROR: Failed to load functions from ${functionsFilePath}: ${error}`);
            process.exit(1);
        }
    }

    // Validate required exports from the loaded module
    // Ensure testData is present (can be null/undefined) and implementations is an object.
    if (loadedModule.testData === undefined || 
        !loadedModule.implementations || typeof loadedModule.implementations !== 'object') {
        // Corrected Error Message: Only mention missing testData or implementations
        console.error(`BENCHMARK_ERROR: Loaded module from ${functionsFilePath} is missing required exports (testData, implementations).`); 
        process.exit(1);
    }

    console.log('[BenchmarkRunner] Validated required exports (testData, implementations).');

    const testData = loadedModule.testData;
    const implementations = loadedModule.implementations as Record<string, string>; // Keep type assertion for TS

    // Find all implementation keys (e.g., 'Original', 'Alternative_1')
    const implementationKeys = Object.keys(implementations);

    if (implementationKeys.length === 0) {
      console.error(`BENCHMARK_ERROR: No implementations found in the loaded module from ${functionsFilePath}`);
      process.exit(1);
    }

    console.log('[BenchmarkRunner] Found implementation keys:', implementationKeys.join(', '));

    // --- START: Prepare functions and arguments outside the loop ---
    // !!! CRITICAL LOGIC: Argument Determination !!!
    // Determines the arguments (`argsForRun`) for the benchmarked functions ONCE
    // based on the structure of `testData`.
    // REASON: Avoids repeated, expensive argument parsing/determination inside 
    //         Benny's timed measurement loop, ensuring only function execution is measured.
    // DO NOT MODIFY without understanding the impact on benchmark accuracy.
    let argsForRun: any[] = [];

    // Determine arguments once - CRITICAL: Avoids re-calculating in the timed loop.
    try {
        // +++ REFACTOR: Use the helper function +++
        argsForRun = determineArguments(testData);
        // +++ END REFACTOR +++
        console.log('[BenchmarkRunner] Determined argsForRun:', JSON.stringify(argsForRun));
    } catch (argError) {
        // +++ DEBUG LOG +++
        console.error(`[BenchmarkRunner DEBUG] Caught error during argument determination: ${argError}`);
        console.error(`BENCHMARK_ERROR: Failed to determine arguments from testData: ${argError}`);
        process.exit(1);
    }

    // +++ REFACTOR: Use the helper function to compile implementations +++
    // Ensure preparedFunctions is declared here to receive the result
    let preparedFunctions: Map<string, (...args: any[]) => any>; 
    try {
        preparedFunctions = compileImplementations(implementations, implementationKeys);
    } catch (compileError) {
         // Error already logged by helper, just exit
         process.exit(1);
    }
    // +++ END REFACTOR +++

    // !!! CRITICAL LOGIC: Function Pre-compilation !!!
    // Compiles each function implementation string using `vm` ONCE and stores
    // the function reference in `preparedFunctions`.
    // REASON: Avoids the extremely expensive overhead of parsing and compiling 
    //         the function code string on every iteration of Benny's measurement cycle.
    // DO NOT MODIFY without understanding the impact on benchmark accuracy.
    console.log('[BenchmarkRunner] Pre-compiling functions...');
    for (const implKey of implementationKeys) {
        try {
            const context = {
                // Include necessary globals if functions depend on them, but NOT testData here
                 console: { log: () => {}, warn: () => {}, error: () => {} },
                 math: Math
            };
            vm.createContext(context);
            // Run the code to define the function in the context
            vm.runInContext(implementations[implKey], context, { timeout: 1000 });
            // Get the function reference
            const funcRef = vm.runInContext(implKey, context);
            if (typeof funcRef !== 'function') {
                throw new Error(`Implementation '${implKey}' did not evaluate to a function.`);
            }
            preparedFunctions.set(implKey, funcRef);
            console.log(`[BenchmarkRunner] Successfully compiled: ${implKey}`);
        } catch(compileError) {
            console.error(`BENCHMARK_ERROR: Failed to compile function '${implKey}': ${compileError}`);
            process.exit(1);
        }
    }
    console.log('[BenchmarkRunner] All functions pre-compiled.');
    // --- END: Prepare functions and arguments outside the loop ---

    // Revert to passing handlers as arguments to benny.suite
    try {
        console.log('[BenchmarkRunner] Setting up Benny suite using argument handlers...');
        // !!! CRITICAL SECTION: BENCHMARK EXECUTION AND OUTPUT FORMAT !!!
        // The structure here (passing handlers as arguments) and the specific
        // format of console.log in benny.cycle and benny.complete are critical.
        benny.suite( 
            'Function Performance Benchmark',
            // Map over implementation keys ('Original', 'Alternative 1', ...)
            ...implementationKeys.map(implKey =>
                 // Return the benny.add() call from the map function
                 benny.add(implKey, () => {
                    // --- START: benny.add callback --- 
                    // !!! CRITICAL LOGIC: Timed Execution !!!
                    // This callback is run repeatedly by Benny for timing.
                    // It MUST remain minimal: only retrieve the pre-compiled function 
                    // and call it with pre-determined args.
                    // REASON: Ensures that only the function's execution time is measured.
                    // DO NOT ADD vm context creation, code execution, or argument parsing here.
                    const funcToRun = preparedFunctions.get(implKey);
                    if (typeof funcToRun !== 'function') {
                        const errorMsg = `BENCHMARK_ITERATION_ERROR: Pre-compiled function not found for key: ${implKey}`;
                        console.error(errorMsg);
                        throw new Error(errorMsg);
                    }
                    try {
                        funcToRun(...argsForRun);
                    } catch (execError) {
                         throw execError; 
                    }
                    // --- END: benny.add callback --- 
                }) // End of benny.add() call
            ), // End of .map()

            // !!! CRITICAL HANDLER: benny.cycle !!!
            // This handler MUST output the cycle results in the EXACT format:
            // "cycle: Name: <name>, Ops: <ops>"
            // REASON: `benchmarkService.ts` uses a specific regex in 
            //         `parseTextBenchmarkOutput` to parse this exact string.
            // DO NOT CHANGE THE OUTPUT FORMAT.
            benny.cycle((event: any) => {
                 // Output the cycle info in the required format
                 console.log(`cycle: Name: ${event.name}, Ops: ${event.ops}`); 
            }),
             // !!! CRITICAL HANDLER: benny.complete !!!
            // This handler MUST output the final fastest result in the EXACT format:
            // "complete: Fastest is <name>" (or "complete: Fastest is Unknown")
            // REASON: `benchmarkService.ts` uses a specific regex in 
            //         `parseTextBenchmarkOutput` to parse this exact string.
            // DO NOT CHANGE THE OUTPUT FORMAT.
            benny.complete((summary: any) => {
                // Log the raw summary for potential debugging
                console.error('[BenchmarkRunner COMPLETE] Benchmark finished. Processing summary...');
                console.error(`[BenchmarkRunner COMPLETE] Raw summary: ${JSON.stringify(summary)}`);

                // Find the fastest result (logic remains the same)
                const fastest = summary.results.reduce((fastest: any, current: any) => {
                    return (!fastest || current.ops > fastest.ops) ? current : fastest;
                }, null);

                if (fastest) {
                    console.log(`complete: Fastest is ${fastest.name}`);
                } else {
                    console.error('[BenchmarkRunner COMPLETE] Could not determine fastest implementation from summary.');
                    console.log('complete: Fastest is Unknown'); 
                }
            })
            // !!! END CRITICAL SECTION: BENCHMARK EXECUTION AND OUTPUT FORMAT !!!
        ); // End of benny.suite() call

        console.log('[BenchmarkRunner] Benny suite setup complete (argument handler style). Run is implicit.');
        // No explicit .run() needed when handlers are passed as arguments

    } catch (error) {
        // Keep the specific error log for setup/run issues
        console.error(`[BenchmarkRunner ERROR] Error during benchmark suite execution (argument handler style): ${error instanceof Error ? error.message : String(error)}`);
        console.error(error instanceof Error ? error.stack : '');
        process.exit(1);
    }
}

// Main execution
// Only run main logic if executed directly
if (require.main === module) {
    const filePath = process.argv[2];
    if (!filePath) {
        console.error('[BenchmarkRunner ERROR] Benchmark file path argument is required.');
        process.exit(1);
    }

    (async () => {
        try {
            await runBenchmarks(filePath);
            // Exit normally if benchmark completes
            // process.exit(0); // Generally not needed unless coordinating multiple processes
        } catch (error) {
            // Error should have been logged within runBenchmarks or helpers
            // We exit here to signal failure to the parent process
             console.error(`[BenchmarkRunner FATAL] Top-level execution error: ${error instanceof Error ? error.message : String(error)}`);
            process.exit(1); // Ensure exit code indicates failure
        }
    })();
} else {
     // This block runs if the file is imported/required
     console.log('[BenchmarkRunner] Module imported, not executing main block.');
     // Exports (like determineArguments, compileImplementations) are available
}